{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051b7a28",
   "metadata": {},
   "source": [
    "# Individual Classifier for Datasets: FGVC & Naturalist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261701d",
   "metadata": {},
   "source": [
    "*Code Writer: Chaeeun Ryu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcaf402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 18 09:11:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   29C    P8    20W / 300W |   1315MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:25:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    26W / 300W |    396MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    19W / 300W |  18225MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    14W / 300W |    789MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    24W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    23W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   29C    P8    26W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    23W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce79287",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d366e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter, OrderedDict\n",
    "import torch.optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import os\n",
    "from torchvision.datasets import FGVCAircraft\n",
    "from torchvision.datasets import INaturalist\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import torchvision.models as models\n",
    "import glob\n",
    "import copy\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "device =  torch.device('cuda:4')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6c25e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c90960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01233d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import copy\n",
    "def load_pretrained_weight(model):\n",
    "    pretrained_model = models.resnet50(pretrained=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    model_weight = copy.deepcopy(model.state_dict())\n",
    "    pretrained_model_weight = copy.deepcopy(pretrained_model.state_dict())\n",
    "    idx = 0\n",
    "    for name, pretrained_weight in zip(model.state_dict().keys(),pretrained_model_weight.values()):\n",
    "        if idx == 318 or idx == 319:\n",
    "            pass\n",
    "        else:\n",
    "            print(name)\n",
    "            assert model_weight[name].size() == pretrained_weight.size()\n",
    "            model_weight[name] = pretrained_weight\n",
    "        idx+=1\n",
    "    model.load_state_dict(model_weight)\n",
    "    idx = 0\n",
    "    for name, pretrained_weight in zip(model.state_dict().keys(),pretrained_model.state_dict().values()):\n",
    "        if torch.equal(model.state_dict()[name],pretrained_weight) == True:\n",
    "            pass\n",
    "        else:\n",
    "            print(idx)\n",
    "    #         model.state_dict()[name] = pretrained_weight\n",
    "        idx+=1\n",
    "    print(\"successfully loaded\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a95b",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41deb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "global args\n",
    "@dataclass\n",
    "class ARGS():\n",
    "    LR = 0.005\n",
    "    EPOCHS = 200\n",
    "    BATCHSIZE = 500\n",
    "    MOMENTUM = 0.9\n",
    "    WORKERS = 0\n",
    "    WEIGHTDECAY = 0\n",
    "    T_MAX = 150\n",
    "    CLS_CLASS = 37\n",
    "    SEG_CLASS = 3\n",
    "    SIZE = 224\n",
    "    SEED = 38\n",
    "    MAX_PATIENCE = 15\n",
    "    \n",
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff04d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021ee8",
   "metadata": {},
   "source": [
    "## Global Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9952b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize((args.SIZE,args.SIZE)),\n",
    "        transforms.RandomCrop(args.SIZE, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.Resize((args.SIZE,args.SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f54685",
   "metadata": {},
   "source": [
    "# FGVCAircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c41553",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_dataset_variant = FGVCAircraft(root='./data', split=\"trainval\", annotation_level  = 'variant',download=True, transform=transform_train)\n",
    "classification_val_dataset_variant = FGVCAircraft(root='./data', split=\"test\", annotation_level  = 'variant',download=True, transform=transform_val)\n",
    "\n",
    "classification_train_dataset_family = FGVCAircraft(root='./data', split=\"trainval\", annotation_level  = 'family',download=True, transform=transform_train)\n",
    "classification_val_dataset_family = FGVCAircraft(root='./data', split=\"test\", annotation_level  = 'family',download=True, transform=transform_val)\n",
    "\n",
    "classification_train_dataset_manufacturer = FGVCAircraft(root='./data', split=\"trainval\", annotation_level  = 'manufacturer',download=True, transform=transform_train)\n",
    "classification_val_dataset_manufacturer = FGVCAircraft(root='./data', split=\"test\", annotation_level  = 'manufacturer',download=True, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9075b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classification_train_dataset_variant.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a93b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_classes\n",
    "data_classes = dict()\n",
    "data_classes['variant'] = len(classification_train_dataset_variant.classes)\n",
    "data_classes['family'] = len(classification_train_dataset_family.classes)\n",
    "data_classes['manufacturer'] = len(classification_train_dataset_manufacturer.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f4ab587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variant': 100, 'family': 70, 'manufacturer': 30}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15688ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_loaders\n",
    "data_loaders = dict()\n",
    "d_name = \"variant\"\n",
    "variant_train_loader = torch.utils.data.DataLoader(\n",
    "    classification_train_dataset_variant, batch_size=args.BATCHSIZE, shuffle=True,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "variant_val_loader = torch.utils.data.DataLoader(\n",
    "    classification_val_dataset_variant, batch_size=args.BATCHSIZE, shuffle=False,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "data_loaders[d_name] = {'train_loader':variant_train_loader,'val_loader':variant_val_loader}\n",
    "\n",
    "d_name = \"family\"\n",
    "family_train_loader = torch.utils.data.DataLoader(\n",
    "    classification_train_dataset_family, batch_size=args.BATCHSIZE, shuffle=True,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "family_val_loader = torch.utils.data.DataLoader(\n",
    "    classification_val_dataset_family, batch_size=args.BATCHSIZE, shuffle=False,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "data_loaders[d_name] = {'train_loader':family_train_loader,'val_loader':family_val_loader}\n",
    "\n",
    "d_name = \"manufacturer\"\n",
    "manufacturer_train_loader = torch.utils.data.DataLoader(\n",
    "    classification_train_dataset_manufacturer, batch_size=args.BATCHSIZE, shuffle=True,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "manufacturer_val_loader = torch.utils.data.DataLoader(\n",
    "    classification_val_dataset_manufacturer, batch_size=args.BATCHSIZE, shuffle=False,\n",
    "    num_workers=args.WORKERS, pin_memory=True)\n",
    "data_loaders[d_name] = {'train_loader':manufacturer_train_loader,'val_loader':manufacturer_val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c409c600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['variant', 'family', 'manufacturer'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c99304",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d3fba",
   "metadata": {},
   "source": [
    "## FGVC 1. Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead0069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels for data variant: 100\n"
     ]
    }
   ],
   "source": [
    "d_name = 'variant'\n",
    "n_label = data_classes[d_name]\n",
    "print(f\"number of labels for data {d_name}: {n_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471d6056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "Reference:\n",
    "https://github.com/kuangliu/pytorch-cifar\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "        self.relu = nn.ReLU(inplace = True) \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=args.CLS_CLASS):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         print(\"block.expansion:\",block.expansion) == 1\n",
    "#         self.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "#         self.linear = nn.Linear(8192, num_classes)\n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out1 = self.layer1(out)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = self.layer4(out3)\n",
    "#         out = F.avg_pool2d(out4, 4)\n",
    "        out = self.avgpool(out4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out#, [out1, out2, out3, out4]\n",
    "\n",
    "\n",
    "def ResNet50(num_classes):\n",
    "    return ResNet(Bottleneck, [3,4,6,3],num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "518cee7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = ResNet50(n_label)\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e04b20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.0.conv3.weight\n",
      "layer1.0.bn3.weight\n",
      "layer1.0.bn3.bias\n",
      "layer1.0.bn3.running_mean\n",
      "layer1.0.bn3.running_var\n",
      "layer1.0.bn3.num_batches_tracked\n",
      "layer1.0.shortcut.0.weight\n",
      "layer1.0.shortcut.1.weight\n",
      "layer1.0.shortcut.1.bias\n",
      "layer1.0.shortcut.1.running_mean\n",
      "layer1.0.shortcut.1.running_var\n",
      "layer1.0.shortcut.1.num_batches_tracked\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.bn1.running_mean\n",
      "layer1.1.bn1.running_var\n",
      "layer1.1.bn1.num_batches_tracked\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer1.1.bn2.running_mean\n",
      "layer1.1.bn2.running_var\n",
      "layer1.1.bn2.num_batches_tracked\n",
      "layer1.1.conv3.weight\n",
      "layer1.1.bn3.weight\n",
      "layer1.1.bn3.bias\n",
      "layer1.1.bn3.running_mean\n",
      "layer1.1.bn3.running_var\n",
      "layer1.1.bn3.num_batches_tracked\n",
      "layer1.2.conv1.weight\n",
      "layer1.2.bn1.weight\n",
      "layer1.2.bn1.bias\n",
      "layer1.2.bn1.running_mean\n",
      "layer1.2.bn1.running_var\n",
      "layer1.2.bn1.num_batches_tracked\n",
      "layer1.2.conv2.weight\n",
      "layer1.2.bn2.weight\n",
      "layer1.2.bn2.bias\n",
      "layer1.2.bn2.running_mean\n",
      "layer1.2.bn2.running_var\n",
      "layer1.2.bn2.num_batches_tracked\n",
      "layer1.2.conv3.weight\n",
      "layer1.2.bn3.weight\n",
      "layer1.2.bn3.bias\n",
      "layer1.2.bn3.running_mean\n",
      "layer1.2.bn3.running_var\n",
      "layer1.2.bn3.num_batches_tracked\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.conv3.weight\n",
      "layer2.0.bn3.weight\n",
      "layer2.0.bn3.bias\n",
      "layer2.0.bn3.running_mean\n",
      "layer2.0.bn3.running_var\n",
      "layer2.0.bn3.num_batches_tracked\n",
      "layer2.0.shortcut.0.weight\n",
      "layer2.0.shortcut.1.weight\n",
      "layer2.0.shortcut.1.bias\n",
      "layer2.0.shortcut.1.running_mean\n",
      "layer2.0.shortcut.1.running_var\n",
      "layer2.0.shortcut.1.num_batches_tracked\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.bn1.running_mean\n",
      "layer2.1.bn1.running_var\n",
      "layer2.1.bn1.num_batches_tracked\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer2.1.bn2.running_mean\n",
      "layer2.1.bn2.running_var\n",
      "layer2.1.bn2.num_batches_tracked\n",
      "layer2.1.conv3.weight\n",
      "layer2.1.bn3.weight\n",
      "layer2.1.bn3.bias\n",
      "layer2.1.bn3.running_mean\n",
      "layer2.1.bn3.running_var\n",
      "layer2.1.bn3.num_batches_tracked\n",
      "layer2.2.conv1.weight\n",
      "layer2.2.bn1.weight\n",
      "layer2.2.bn1.bias\n",
      "layer2.2.bn1.running_mean\n",
      "layer2.2.bn1.running_var\n",
      "layer2.2.bn1.num_batches_tracked\n",
      "layer2.2.conv2.weight\n",
      "layer2.2.bn2.weight\n",
      "layer2.2.bn2.bias\n",
      "layer2.2.bn2.running_mean\n",
      "layer2.2.bn2.running_var\n",
      "layer2.2.bn2.num_batches_tracked\n",
      "layer2.2.conv3.weight\n",
      "layer2.2.bn3.weight\n",
      "layer2.2.bn3.bias\n",
      "layer2.2.bn3.running_mean\n",
      "layer2.2.bn3.running_var\n",
      "layer2.2.bn3.num_batches_tracked\n",
      "layer2.3.conv1.weight\n",
      "layer2.3.bn1.weight\n",
      "layer2.3.bn1.bias\n",
      "layer2.3.bn1.running_mean\n",
      "layer2.3.bn1.running_var\n",
      "layer2.3.bn1.num_batches_tracked\n",
      "layer2.3.conv2.weight\n",
      "layer2.3.bn2.weight\n",
      "layer2.3.bn2.bias\n",
      "layer2.3.bn2.running_mean\n",
      "layer2.3.bn2.running_var\n",
      "layer2.3.bn2.num_batches_tracked\n",
      "layer2.3.conv3.weight\n",
      "layer2.3.bn3.weight\n",
      "layer2.3.bn3.bias\n",
      "layer2.3.bn3.running_mean\n",
      "layer2.3.bn3.running_var\n",
      "layer2.3.bn3.num_batches_tracked\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.conv3.weight\n",
      "layer3.0.bn3.weight\n",
      "layer3.0.bn3.bias\n",
      "layer3.0.bn3.running_mean\n",
      "layer3.0.bn3.running_var\n",
      "layer3.0.bn3.num_batches_tracked\n",
      "layer3.0.shortcut.0.weight\n",
      "layer3.0.shortcut.1.weight\n",
      "layer3.0.shortcut.1.bias\n",
      "layer3.0.shortcut.1.running_mean\n",
      "layer3.0.shortcut.1.running_var\n",
      "layer3.0.shortcut.1.num_batches_tracked\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.bn1.running_mean\n",
      "layer3.1.bn1.running_var\n",
      "layer3.1.bn1.num_batches_tracked\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer3.1.bn2.running_mean\n",
      "layer3.1.bn2.running_var\n",
      "layer3.1.bn2.num_batches_tracked\n",
      "layer3.1.conv3.weight\n",
      "layer3.1.bn3.weight\n",
      "layer3.1.bn3.bias\n",
      "layer3.1.bn3.running_mean\n",
      "layer3.1.bn3.running_var\n",
      "layer3.1.bn3.num_batches_tracked\n",
      "layer3.2.conv1.weight\n",
      "layer3.2.bn1.weight\n",
      "layer3.2.bn1.bias\n",
      "layer3.2.bn1.running_mean\n",
      "layer3.2.bn1.running_var\n",
      "layer3.2.bn1.num_batches_tracked\n",
      "layer3.2.conv2.weight\n",
      "layer3.2.bn2.weight\n",
      "layer3.2.bn2.bias\n",
      "layer3.2.bn2.running_mean\n",
      "layer3.2.bn2.running_var\n",
      "layer3.2.bn2.num_batches_tracked\n",
      "layer3.2.conv3.weight\n",
      "layer3.2.bn3.weight\n",
      "layer3.2.bn3.bias\n",
      "layer3.2.bn3.running_mean\n",
      "layer3.2.bn3.running_var\n",
      "layer3.2.bn3.num_batches_tracked\n",
      "layer3.3.conv1.weight\n",
      "layer3.3.bn1.weight\n",
      "layer3.3.bn1.bias\n",
      "layer3.3.bn1.running_mean\n",
      "layer3.3.bn1.running_var\n",
      "layer3.3.bn1.num_batches_tracked\n",
      "layer3.3.conv2.weight\n",
      "layer3.3.bn2.weight\n",
      "layer3.3.bn2.bias\n",
      "layer3.3.bn2.running_mean\n",
      "layer3.3.bn2.running_var\n",
      "layer3.3.bn2.num_batches_tracked\n",
      "layer3.3.conv3.weight\n",
      "layer3.3.bn3.weight\n",
      "layer3.3.bn3.bias\n",
      "layer3.3.bn3.running_mean\n",
      "layer3.3.bn3.running_var\n",
      "layer3.3.bn3.num_batches_tracked\n",
      "layer3.4.conv1.weight\n",
      "layer3.4.bn1.weight\n",
      "layer3.4.bn1.bias\n",
      "layer3.4.bn1.running_mean\n",
      "layer3.4.bn1.running_var\n",
      "layer3.4.bn1.num_batches_tracked\n",
      "layer3.4.conv2.weight\n",
      "layer3.4.bn2.weight\n",
      "layer3.4.bn2.bias\n",
      "layer3.4.bn2.running_mean\n",
      "layer3.4.bn2.running_var\n",
      "layer3.4.bn2.num_batches_tracked\n",
      "layer3.4.conv3.weight\n",
      "layer3.4.bn3.weight\n",
      "layer3.4.bn3.bias\n",
      "layer3.4.bn3.running_mean\n",
      "layer3.4.bn3.running_var\n",
      "layer3.4.bn3.num_batches_tracked\n",
      "layer3.5.conv1.weight\n",
      "layer3.5.bn1.weight\n",
      "layer3.5.bn1.bias\n",
      "layer3.5.bn1.running_mean\n",
      "layer3.5.bn1.running_var\n",
      "layer3.5.bn1.num_batches_tracked\n",
      "layer3.5.conv2.weight\n",
      "layer3.5.bn2.weight\n",
      "layer3.5.bn2.bias\n",
      "layer3.5.bn2.running_mean\n",
      "layer3.5.bn2.running_var\n",
      "layer3.5.bn2.num_batches_tracked\n",
      "layer3.5.conv3.weight\n",
      "layer3.5.bn3.weight\n",
      "layer3.5.bn3.bias\n",
      "layer3.5.bn3.running_mean\n",
      "layer3.5.bn3.running_var\n",
      "layer3.5.bn3.num_batches_tracked\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.conv3.weight\n",
      "layer4.0.bn3.weight\n",
      "layer4.0.bn3.bias\n",
      "layer4.0.bn3.running_mean\n",
      "layer4.0.bn3.running_var\n",
      "layer4.0.bn3.num_batches_tracked\n",
      "layer4.0.shortcut.0.weight\n",
      "layer4.0.shortcut.1.weight\n",
      "layer4.0.shortcut.1.bias\n",
      "layer4.0.shortcut.1.running_mean\n",
      "layer4.0.shortcut.1.running_var\n",
      "layer4.0.shortcut.1.num_batches_tracked\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.bn1.running_mean\n",
      "layer4.1.bn1.running_var\n",
      "layer4.1.bn1.num_batches_tracked\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "layer4.1.bn2.running_mean\n",
      "layer4.1.bn2.running_var\n",
      "layer4.1.bn2.num_batches_tracked\n",
      "layer4.1.conv3.weight\n",
      "layer4.1.bn3.weight\n",
      "layer4.1.bn3.bias\n",
      "layer4.1.bn3.running_mean\n",
      "layer4.1.bn3.running_var\n",
      "layer4.1.bn3.num_batches_tracked\n",
      "layer4.2.conv1.weight\n",
      "layer4.2.bn1.weight\n",
      "layer4.2.bn1.bias\n",
      "layer4.2.bn1.running_mean\n",
      "layer4.2.bn1.running_var\n",
      "layer4.2.bn1.num_batches_tracked\n",
      "layer4.2.conv2.weight\n",
      "layer4.2.bn2.weight\n",
      "layer4.2.bn2.bias\n",
      "layer4.2.bn2.running_mean\n",
      "layer4.2.bn2.running_var\n",
      "layer4.2.bn2.num_batches_tracked\n",
      "layer4.2.conv3.weight\n",
      "layer4.2.bn3.weight\n",
      "layer4.2.bn3.bias\n",
      "layer4.2.bn3.running_mean\n",
      "layer4.2.bn3.running_var\n",
      "layer4.2.bn3.num_batches_tracked\n",
      "318\n",
      "319\n",
      "successfully loaded\n"
     ]
    }
   ],
   "source": [
    "resnet50 = load_pretrained_weight(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "896386a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cb4ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cls(epoch,train_loader,model,seg_criterion, optimizer, device=device, cls_criterion = cls_criterion):\n",
    "    assert seg_criterion == None\n",
    "    ss_losses = []\n",
    "    save_ss_acc = []\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    if epoch == 0:\n",
    "        for image, label in tqdm(train_loader):\n",
    "            image,label = image.to(device), label.to(device)\n",
    "            surrogate_out = model(image)\n",
    "            surrogate_pred = torch.argmax(surrogate_out,axis = 1)\n",
    "            cls_loss = cls_criterion(surrogate_out, label)\n",
    "            loss = cls_loss\n",
    "            ss_losses.append(cls_loss.item())\n",
    "            acc1 = (torch.sum((surrogate_pred==label)*1)/len(label)).item()\n",
    "            save_ss_acc.append(acc1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return np.mean(ss_losses),np.mean(save_ss_acc)\n",
    "    else:\n",
    "        for image, label in train_loader:\n",
    "            image,label = image.to(device), label.to(device)\n",
    "            surrogate_out = model(image)\n",
    "            surrogate_pred = torch.argmax(surrogate_out,axis = 1)\n",
    "            cls_loss = cls_criterion(surrogate_out, label)\n",
    "            loss = cls_loss\n",
    "            ss_losses.append(cls_loss.item())\n",
    "            acc1 = (torch.sum((surrogate_pred==label)*1)/len(label)).item()\n",
    "            save_ss_acc.append(acc1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return np.mean(ss_losses),np.mean(save_ss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af6edbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cls(val_loader, model, seg_criterion, device, cls_criterion = cls_criterion):\n",
    "    assert seg_criterion == None\n",
    "    ss_losses = [] \n",
    "    ss_acc = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image,label in val_loader:\n",
    "            image,label = image.to(device),label.to(device)\n",
    "            surrogate_out = model(image)\n",
    "            surrogate_pred = torch.argmax(surrogate_out, axis = 1)\n",
    "            acc1 = (torch.sum((surrogate_pred==label)*1)/len(surrogate_pred)).item()\n",
    "            ss_acc.append(acc1)\n",
    "            \n",
    "            cls_loss = cls_criterion(surrogate_out,label)\n",
    "            loss = cls_loss\n",
    "            ss_losses.append(cls_loss.item())\n",
    "            \n",
    "    return np.mean(ss_losses), np.mean(ss_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a668ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_unet_optimizer = torch.optim.SGD(resnet50.parameters(), args.LR,\n",
    "                            momentum=args.MOMENTUM,\n",
    "                            weight_decay=args.WEIGHTDECAY)\n",
    "ch_unet_scheduler = CosineAnnealingLR(ch_unet_optimizer, T_max = args.T_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7dc18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starting..\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005812883377075195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 19,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9ec524f5a54fc18e778ad4131e862b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005503177642822266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 19,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 14,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4c8e6a8e414cd5a811eb82fd06b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "save_ss_train_loss = []\n",
    "save_ss_val_loss = []\n",
    "save_ss_train_acc = []\n",
    "save_ss_val_acc = []\n",
    "\n",
    "ss_best_val_acc = 0.\n",
    "\n",
    "that_time_acc= None\n",
    "max_patience = args.MAX_PATIENCE\n",
    "resnet50.to(device)\n",
    "seg_criterion = None\n",
    "\n",
    "print(\"training starting..\")\n",
    "\n",
    "for epoch in tqdm(range(args.EPOCHS)):\n",
    "    ss_train_loss,ss_train_acc = train_cls(epoch,data_loaders[d_name]['train_loader'], resnet50, seg_criterion, ch_unet_optimizer,device, cls_criterion)\n",
    "    ss_val_loss,ss_val_acc = eval_cls(data_loaders[d_name]['val_loader'], resnet50, seg_criterion, device, cls_criterion)\n",
    "    \n",
    "    #self-supervision\n",
    "    save_ss_train_loss.append(ss_train_loss)\n",
    "    save_ss_train_acc.append(ss_train_acc)\n",
    "    save_ss_val_loss.append(ss_val_loss)\n",
    "    save_ss_val_acc.append(ss_val_acc)\n",
    "    \n",
    "    print(f\"======= epoch {epoch} ========\")\n",
    "    print(\"avg train acc:\",ss_train_acc,\"avg val acc:\",ss_val_acc, \"ss train loss:\",ss_train_loss, \"ss val loss:\",ss_val_loss)\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        print(f\"best average acc so far:{ss_best_val_acc}\")\n",
    "    if ss_val_acc > ss_best_val_acc:\n",
    "        print(\"model updated at epoch:\",epoch,\"avg val acc:\",ss_val_acc)\n",
    "        best_model = copy.deepcopy(resnet50)\n",
    "        ss_best_val_acc = ss_val_acc\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience +=1\n",
    "    \n",
    "    if patience > max_patience:\n",
    "        print(\"patience overloaded..! stop learning.........\")\n",
    "        break\n",
    "        \n",
    "    ch_unet_scheduler.step()\n",
    "\n",
    "# high score: 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(save_ss_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = dict()\n",
    "# log_dict['save_train_loss'] = save_train_loss\n",
    "# log_dict['save_val_loss'] = save_val_loss\n",
    "# log_dict['save_train_dice'] = save_train_dice\n",
    "# log_dict['save_val_dice'] = save_val_dice\n",
    "log_dict['save_ss_train_acc'] = save_ss_train_acc\n",
    "log_dict['save_ss_val_acc'] = save_ss_val_acc\n",
    "log_dict['save_ss_train_loss'] = save_ss_train_loss\n",
    "log_dict['save_ss_val_loss'] = save_ss_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac042e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(log_dict,f\"./{d_name}_resnet50_only_cls_log.pt\")\n",
    "torch.save({\"model_w\":resnet50.state_dict(),\"best_model_w\":best_model.state_dict()},f\"./{d_name}resnet50_only_cls_w.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Saved loss for only classification (ResNet50)\")\n",
    "plt.plot(save_ss_train_loss,label = \"train loss\")\n",
    "plt.plot(save_ss_val_loss, label = \"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02469a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Saved acc for only classification (ResNet50)\")\n",
    "plt.plot(save_ss_train_acc,label = \"train\")\n",
    "plt.plot(save_ss_val_acc,label = \"valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627889b7",
   "metadata": {},
   "source": [
    "## FGVC 2. Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = 'family'\n",
    "n_label = data_classes[d_name]\n",
    "print(f\"number of labels for data {d_name}: {n_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50(n_label)\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a11f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_unet_optimizer = torch.optim.SGD(resnet50.parameters(), args.LR,\n",
    "                            momentum=args.MOMENTUM,\n",
    "                            weight_decay=args.WEIGHTDECAY)\n",
    "ch_unet_scheduler = CosineAnnealingLR(ch_unet_optimizer, T_max = args.T_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "save_ss_train_loss = []\n",
    "save_ss_val_loss = []\n",
    "save_ss_train_acc = []\n",
    "save_ss_val_acc = []\n",
    "\n",
    "ss_best_val_acc = 0.\n",
    "\n",
    "that_time_acc= None\n",
    "max_patience = args.MAX_PATIENCE\n",
    "resnet50.to(device)\n",
    "seg_criterion = None\n",
    "\n",
    "print(\"training starting..\")\n",
    "\n",
    "for epoch in tqdm(range(args.EPOCHS)):\n",
    "    ss_train_loss,ss_train_acc = train_cls(epoch,data_loaders[d_name]['train_loader'], resnet50, seg_criterion, ch_unet_optimizer,device, cls_criterion)\n",
    "    ss_val_loss,ss_val_acc = eval_cls(data_loaders[d_name]['val_loader'], resnet50, seg_criterion, device, cls_criterion)\n",
    "    \n",
    "    #self-supervision\n",
    "    save_ss_train_loss.append(ss_train_loss)\n",
    "    save_ss_train_acc.append(ss_train_acc)\n",
    "    save_ss_val_loss.append(ss_val_loss)\n",
    "    save_ss_val_acc.append(ss_val_acc)\n",
    "    \n",
    "    print(f\"======= epoch {epoch} ========\")\n",
    "    print(\"avg train acc:\",ss_train_acc,\"avg val acc:\",ss_val_acc, \"ss train loss:\",ss_train_loss, \"ss val loss:\",ss_val_loss)\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        print(f\"best average acc so far:{ss_best_val_acc}\")\n",
    "    if ss_val_acc > ss_best_val_acc:\n",
    "        print(\"model updated at epoch:\",epoch,\"avg val acc:\",ss_val_acc)\n",
    "        best_model = copy.deepcopy(resnet50)\n",
    "        ss_best_val_acc = ss_val_acc\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience +=1\n",
    "    \n",
    "    if patience > max_patience:\n",
    "        print(\"patience overloaded..! stop learning.........\")\n",
    "        break\n",
    "        \n",
    "    ch_unet_scheduler.step()\n",
    "\n",
    "# high score: 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d925c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(save_ss_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = dict()\n",
    "# log_dict['save_train_loss'] = save_train_loss\n",
    "# log_dict['save_val_loss'] = save_val_loss\n",
    "# log_dict['save_train_dice'] = save_train_dice\n",
    "# log_dict['save_val_dice'] = save_val_dice\n",
    "log_dict['save_ss_train_acc'] = save_ss_train_acc\n",
    "log_dict['save_ss_val_acc'] = save_ss_val_acc\n",
    "log_dict['save_ss_train_loss'] = save_ss_train_loss\n",
    "log_dict['save_ss_val_loss'] = save_ss_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885713a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(log_dict,f\"./{d_name}_resnet50_only_cls_log.pt\")\n",
    "torch.save({\"model_w\":resnet50.state_dict(),\"best_model_w\":best_model.state_dict()},f\"./{d_name}resnet50_only_cls_w.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07627121",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Saved loss for only classification (ResNet50) ({d_name})\")\n",
    "plt.plot(save_ss_train_loss,label = \"train loss\")\n",
    "plt.plot(save_ss_val_loss, label = \"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Saved acc for only classification (ResNet50) ({d_name})\")\n",
    "plt.plot(save_ss_train_acc,label = \"train\")\n",
    "plt.plot(save_ss_val_acc,label = \"valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8165b34",
   "metadata": {},
   "source": [
    "## FGVC 3. manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = 'manufacturer'\n",
    "n_label = data_classes[d_name]\n",
    "print(f\"number of labels for data {d_name}: {n_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50(n_label)\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_unet_optimizer = torch.optim.SGD(resnet50.parameters(), args.LR,\n",
    "                            momentum=args.MOMENTUM,\n",
    "                            weight_decay=args.WEIGHTDECAY)\n",
    "ch_unet_scheduler = CosineAnnealingLR(ch_unet_optimizer, T_max = args.T_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f755395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "save_ss_train_loss = []\n",
    "save_ss_val_loss = []\n",
    "save_ss_train_acc = []\n",
    "save_ss_val_acc = []\n",
    "\n",
    "ss_best_val_acc = 0.\n",
    "\n",
    "that_time_acc= None\n",
    "max_patience = args.MAX_PATIENCE\n",
    "resnet50.to(device)\n",
    "seg_criterion = None\n",
    "\n",
    "print(\"training starting..\")\n",
    "\n",
    "for epoch in tqdm(range(args.EPOCHS)):\n",
    "    ss_train_loss,ss_train_acc = train_cls(epoch,data_loaders[d_name]['train_loader'], resnet50, seg_criterion, ch_unet_optimizer,device, cls_criterion)\n",
    "    ss_val_loss,ss_val_acc = eval_cls(data_loaders[d_name]['val_loader'], resnet50, seg_criterion, device, cls_criterion)\n",
    "    \n",
    "    #self-supervision\n",
    "    save_ss_train_loss.append(ss_train_loss)\n",
    "    save_ss_train_acc.append(ss_train_acc)\n",
    "    save_ss_val_loss.append(ss_val_loss)\n",
    "    save_ss_val_acc.append(ss_val_acc)\n",
    "    \n",
    "    print(f\"======= epoch {epoch} ========\")\n",
    "    print(\"avg train acc:\",ss_train_acc,\"avg val acc:\",ss_val_acc, \"ss train loss:\",ss_train_loss, \"ss val loss:\",ss_val_loss)\n",
    "    \n",
    "    if epoch%20 == 0:\n",
    "        print(f\"best average acc so far:{ss_best_val_acc}\")\n",
    "    if ss_val_acc > ss_best_val_acc:\n",
    "        print(\"model updated at epoch:\",epoch,\"avg val acc:\",ss_val_acc)\n",
    "        best_model = copy.deepcopy(resnet50)\n",
    "        ss_best_val_acc = ss_val_acc\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience +=1\n",
    "    \n",
    "    if patience > max_patience:\n",
    "        print(\"patience overloaded..! stop learning.........\")\n",
    "        break\n",
    "        \n",
    "    ch_unet_scheduler.step()\n",
    "\n",
    "# high score: 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(save_ss_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = dict()\n",
    "# log_dict['save_train_loss'] = save_train_loss\n",
    "# log_dict['save_val_loss'] = save_val_loss\n",
    "# log_dict['save_train_dice'] = save_train_dice\n",
    "# log_dict['save_val_dice'] = save_val_dice\n",
    "log_dict['save_ss_train_acc'] = save_ss_train_acc\n",
    "log_dict['save_ss_val_acc'] = save_ss_val_acc\n",
    "log_dict['save_ss_train_loss'] = save_ss_train_loss\n",
    "log_dict['save_ss_val_loss'] = save_ss_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(log_dict,f\"./{d_name}_resnet50_only_cls_log.pt\")\n",
    "torch.save({\"model_w\":resnet50.state_dict(),\"best_model_w\":best_model.state_dict()},f\"./{d_name}resnet50_only_cls_w.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Saved loss for only classification (ResNet50) ({d_name})\")\n",
    "plt.plot(save_ss_train_loss,label = \"train loss\")\n",
    "plt.plot(save_ss_val_loss, label = \"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a59145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Saved acc for only classification (ResNet50) ({d_name})\")\n",
    "plt.plot(save_ss_train_acc,label = \"train\")\n",
    "plt.plot(save_ss_val_acc,label = \"valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
